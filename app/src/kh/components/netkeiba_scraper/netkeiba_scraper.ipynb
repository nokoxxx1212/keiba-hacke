{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_netkeiba_url_potential(year_start, year_end):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with potential netkeiba URLs for scraping.\n",
    "\n",
    "    Args:\n",
    "        year_start (int): The starting year for scraping.\n",
    "        year_end (int): The ending year for scraping.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the potential netkeiba URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    netkeiba_url_potential_df = pd.DataFrame(columns=['url', 'current_race_id', 'racecourse_i', 'place', 'race_number_i', 'year'])\n",
    "    # ①競馬場ごとにループ（10競馬場）\n",
    "    for year in range(year_start, year_end+1):\n",
    "        # racecourse_list=[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\"]\n",
    "        racecourse_list=[\"01\"]\n",
    "        for racecourse_i in range(len(racecourse_list)):\n",
    "            place = \"\"\n",
    "            if racecourse_list[racecourse_i] == \"01\":\n",
    "                place = \"札幌\"\n",
    "            elif racecourse_list[racecourse_i] == \"02\":\n",
    "                place = \"函館\"\n",
    "            elif racecourse_list[racecourse_i] == \"03\":\n",
    "                place = \"福島\"\n",
    "            elif racecourse_list[racecourse_i] == \"04\":\n",
    "                place = \"新潟\"\n",
    "            elif racecourse_list[racecourse_i] == \"05\":\n",
    "                place = \"東京\"\n",
    "            elif racecourse_list[racecourse_i] == \"06\":\n",
    "                place = \"中山\"\n",
    "            elif racecourse_list[racecourse_i] == \"07\":\n",
    "                place = \"中京\"\n",
    "            elif racecourse_list[racecourse_i] == \"08\":\n",
    "                place = \"京都\"\n",
    "            elif racecourse_list[racecourse_i] == \"09\":\n",
    "                place = \"阪神\"\n",
    "            elif racecourse_list[racecourse_i] == \"10\":\n",
    "                place = \"小倉\"    \n",
    "            # ②開催回数ごとにループ（6回）\n",
    "            for session_number_i in range(6+1):\n",
    "                continueCounter = 0  # 'continue'が実行された回数をカウントするためのカウンターを追加\n",
    "                # ③開催日数分ループ（12日）\n",
    "                for event_date_i in range(12+1):\n",
    "                    race_id = ''\n",
    "                    if event_date_i<9:\n",
    "                        race_id = str(year)+racecourse_list[racecourse_i]+\"0\"+str(session_number_i+1)+\"0\"+str(event_date_i+1)\n",
    "                        url1=\"https://db.netkeiba.com/race/\"+race_id\n",
    "                    else:\n",
    "                        race_id = str(year)+racecourse_list[racecourse_i]+\"0\"+str(session_number_i+1)+\"0\"+str(event_date_i+1)\n",
    "                        url1=\"https://db.netkeiba.com/race/\"+race_id\n",
    "                    # event_date_iの更新をbreakするためのカウンター\n",
    "                    event_date_i_BreakCounter = 0\n",
    "                    # ④レース数分ループ（12R）\n",
    "                    for race_number_i in range(12):\n",
    "                        if race_number_i<9:\n",
    "                            url=url1+str(\"0\")+str(race_number_i+1)\n",
    "                            current_race_id = race_id+str(\"0\")+str(race_number_i+1)\n",
    "                        else:\n",
    "                            url=url1+str(race_number_i+1)\n",
    "                            current_race_id = race_id+str(race_number_i+1)\n",
    "                        # add the URL to the DataFrame\n",
    "                        netkeiba_url_potential_df.loc[len(netkeiba_url_potential_df)] = [url, current_race_id, racecourse_i, place, race_number_i, year]\n",
    "    return netkeiba_url_potential_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得開始年\n",
    "year_start = 2019\n",
    "#取得終了年（含む）\n",
    "year_end = 2019\n",
    "netkeiba_url_potential_df = create_netkeiba_url_potential(year_start, year_end)\n",
    "netkeiba_url_potential_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_read_netkeiba_url_scraped_csv_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"\n",
    "    Download a CSV file from a GCS bucket and read it into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    bucket_name (str): The name of the GCS bucket.\n",
    "    source_blob_name (str): The name of the blob in the GCS bucket.\n",
    "    destination_file_name (str): The name of the file to save the downloaded CSV.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the data from the downloaded CSV file.\n",
    "    \"\"\"\n",
    "    # Create a client\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    # Get the blob\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if blob.exists():\n",
    "        # Download the file\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "\n",
    "        # If the file exists, read it into a DataFrame\n",
    "        if os.path.exists(destination_file_name):\n",
    "            netkeiba_url_scraped_df = pd.read_csv(destination_file_name)\n",
    "            return netkeiba_url_scraped_df\n",
    "    else:\n",
    "        print(f\"The file {source_blob_name} does not exist in the bucket {bucket_name}.\")\n",
    "        # Return an empty DataFrame\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netkeiba_url_scraped_df = download_read_netkeiba_url_scraped_csv_from_gcs(\"dev-kh-gcs-bucket\", \"data/netkeiba_url_scraped.csv\", \"netkeiba_url_scraped.csv\")\n",
    "netkeiba_url_scraped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_scraped_urls(netkeiba_url_potential_df, netkeiba_url_scraped_df):\n",
    "    # Check if netkeiba_url_scraped_df is not None and not empty\n",
    "    if netkeiba_url_scraped_df is not None and not netkeiba_url_scraped_df.empty:\n",
    "        # Remove rows in netkeiba_url_potential_df that are also in netkeiba_url_scraped_df\n",
    "        df_unique = netkeiba_url_potential_df[~netkeiba_url_potential_df['url'].isin(netkeiba_url_scraped_df['url'])]\n",
    "        return df_unique\n",
    "    else:\n",
    "        print(\"netkeiba_url_scraped_df is None or empty. Returning netkeiba_url_potential_df as is.\")\n",
    "        return netkeiba_url_potential_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_scraped_urls(netkeiba_url_potential_df, netkeiba_url_scraped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_netkeiba_data(year_start, year_end):\n",
    "        race_data_all = []\n",
    "        #取得するデータのヘッダー情報を先に追加しておく\n",
    "        race_data_all.append(['race_id','馬','騎手','馬番','走破時間','オッズ','通過順','着順','体重','体重変化','性','齢','斤量','上がり','人気','レース名','日付','開催','クラス','芝・ダート','距離','回り','馬場','天気','場id','場名'])\n",
    "\n",
    "                        try:\n",
    "                            r=requests.get(url)\n",
    "                        #リクエストを投げすぎるとエラーになることがあるため\n",
    "                        #失敗したら10秒待機してリトライする\n",
    "                        except requests.exceptions.RequestException as e:\n",
    "                            print(f\"Error: {e}\")\n",
    "                            print(\"Retrying in 10 seconds...\")\n",
    "                            time.sleep(10)  # 10秒待機\n",
    "                            r=requests.get(url)\n",
    "                        #バグ対策でdecode\n",
    "                        soup = BeautifulSoup(r.content.decode(\"euc-jp\", \"ignore\"), \"html.parser\")\n",
    "                        soup_span = soup.find_all(\"span\")\n",
    "                        # テーブルを指定\n",
    "                        main_table = soup.find(\"table\", {\"class\": \"race_table_01 nk_tb_common\"})\n",
    "    \n",
    "                        # テーブル内の全ての行を取得\n",
    "                        try:\n",
    "                            main_rows = main_table.find_all(\"tr\")\n",
    "                        except:\n",
    "                            print('continue: ' + url)\n",
    "                            continueCounter += 1  # 'continue'が実行された回数をカウントアップ\n",
    "                            if continueCounter == 2:  # 'continue'が2回連続で実行されたらループを抜ける\n",
    "                                continueCounter = 0\n",
    "                                break\n",
    "                            continue\n",
    "    \n",
    "                        race_data = []\n",
    "                        for i, row in enumerate(main_rows[1:], start=1):# ヘッダ行をスキップ\n",
    "                            cols = row.find_all(\"td\")\n",
    "                            #走破時間\n",
    "                            runtime=''\n",
    "                            try:\n",
    "                                runtime= cols[7].text.strip()\n",
    "                            except IndexError:\n",
    "                                runtime = ''\n",
    "                            soup_nowrap = soup.find_all(\"td\",nowrap=\"nowrap\",class_=None)\n",
    "                            #通過順\n",
    "                            pas = ''\n",
    "                            try:\n",
    "                                pas = str(cols[10].text.strip())\n",
    "                            except:\n",
    "                                pas = ''\n",
    "                            weight = 0\n",
    "                            weight_dif = 0\n",
    "                            #体重\n",
    "                            var = cols[14].text.strip()\n",
    "                            try:\n",
    "                                weight = int(var.split(\"(\")[0])\n",
    "                                weight_dif = int(var.split(\"(\")[1][0:-1])\n",
    "                            except ValueError:\n",
    "                                weight = 0\n",
    "                                weight_dif = 0\n",
    "                            weight = weight\n",
    "                            weight_dif = weight_dif\n",
    "                            #上がり\n",
    "                            last = ''\n",
    "                            try:\n",
    "                                last = cols[11].text.strip()\n",
    "                            except IndexError:\n",
    "                                last = ''\n",
    "                            #人気\n",
    "                            pop = ''\n",
    "                            try:\n",
    "                                pop = cols[13].text.strip()\n",
    "                            except IndexError:\n",
    "                                pop = ''\n",
    "                            \n",
    "                            #レースの情報\n",
    "                            try:\n",
    "                                var = soup_span[8]\n",
    "                                sur=str(var).split(\"/\")[0].split(\">\")[1][0]\n",
    "                                rou=str(var).split(\"/\")[0].split(\">\")[1][1]\n",
    "                                dis=str(var).split(\"/\")[0].split(\">\")[1].split(\"m\")[0][-4:]\n",
    "                                con=str(var).split(\"/\")[2].split(\":\")[1][1]\n",
    "                                wed=str(var).split(\"/\")[1].split(\":\")[1][1]\n",
    "                            except IndexError:\n",
    "                                try:\n",
    "                                    var = soup_span[7]\n",
    "                                    sur=str(var).split(\"/\")[0].split(\">\")[1][0]\n",
    "                                    rou=str(var).split(\"/\")[0].split(\">\")[1][1]\n",
    "                                    dis=str(var).split(\"/\")[0].split(\">\")[1].split(\"m\")[0][-4:]\n",
    "                                    con=str(var).split(\"/\")[2].split(\":\")[1][1]\n",
    "                                    wed=str(var).split(\"/\")[1].split(\":\")[1][1]\n",
    "                                except IndexError:\n",
    "                                    var = soup_span[6]\n",
    "                                    sur=str(var).split(\"/\")[0].split(\">\")[1][0]\n",
    "                                    rou=str(var).split(\"/\")[0].split(\">\")[1][1]\n",
    "                                    dis=str(var).split(\"/\")[0].split(\">\")[1].split(\"m\")[0][-4:]\n",
    "                                    con=str(var).split(\"/\")[2].split(\":\")[1][1]\n",
    "                                    wed=str(var).split(\"/\")[1].split(\":\")[1][1]\n",
    "                            soup_smalltxt = soup.find_all(\"p\",class_=\"smalltxt\")\n",
    "                            detail=str(soup_smalltxt).split(\">\")[1].split(\" \")[1]\n",
    "                            date=str(soup_smalltxt).split(\">\")[1].split(\" \")[0]\n",
    "                            clas=str(soup_smalltxt).split(\">\")[1].split(\" \")[2].replace(u'\\xa0', u' ').split(\" \")[0]\n",
    "                            title=str(soup.find_all(\"h1\")[1]).split(\">\")[1].split(\"<\")[0]\n",
    "    \n",
    "                            race_data = [\n",
    "                                current_race_id,\n",
    "                                cols[3].text.strip(),#馬の名前\n",
    "                                cols[6].text.strip(),#騎手の名前\n",
    "                                cols[2].text.strip(),#馬番\n",
    "                                runtime,#走破時間\n",
    "                                cols[12].text.strip(),#オッズ,\n",
    "                                pas,#通過順\n",
    "                                cols[0].text.strip(),#着順\n",
    "                                weight,#体重\n",
    "                                weight_dif,#体重変化\n",
    "                                cols[4].text.strip()[0],#性\n",
    "                                cols[4].text.strip()[1],#齢\n",
    "                                cols[5].text.strip(),#斤量\n",
    "                                last,#上がり\n",
    "                                pop,#人気,\n",
    "                                title,#レース名\n",
    "                                date,#日付\n",
    "                                detail,\n",
    "                                clas,#クラス\n",
    "                                sur,#芝かダートか\n",
    "                                dis,#距離\n",
    "                                rou,#回り\n",
    "                                con,#馬場状態\n",
    "                                wed,#天気\n",
    "                                racecourse_i,#場\n",
    "                                place]\n",
    "                            race_data_all.append(race_data)\n",
    "                        \n",
    "                        print(detail+str(race_number_i+1)+\"R\")#進捗を表示\n",
    "                        \n",
    "                    if event_date_i_BreakCounter == 12:#12レース全部ない日が検出されたら、その開催中の最後の開催日と考える\n",
    "                        break\n",
    "        #1年毎に出力\n",
    "        #出力先とファイル名は修正してください\n",
    "        with open('data/'+str(year)+'.csv', 'w', newline='',encoding=\"SHIFT-JIS\") as f:\n",
    "            csv.writer(f).writerows(race_data_all)\n",
    "        print(\"終了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
