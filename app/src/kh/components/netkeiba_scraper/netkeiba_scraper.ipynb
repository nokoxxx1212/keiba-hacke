{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_netkeiba_url_potential(year_start, year_end):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with potential netkeiba URLs for scraping.\n",
    "\n",
    "    Args:\n",
    "        year_start (int): The starting year for scraping.\n",
    "        year_end (int): The ending year for scraping.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the potential netkeiba URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    netkeiba_url_potential_df = pd.DataFrame(columns=['url', 'current_race_id', 'racecourse_i', 'place', 'race_number_i', 'year'])\n",
    "    # ①競馬場ごとにループ（10競馬場）\n",
    "    for year in range(year_start, year_end+1):\n",
    "        # racecourse_list=[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\"]\n",
    "        racecourse_list=[\"01\"]\n",
    "        for racecourse_i in range(len(racecourse_list)):\n",
    "            place = \"\"\n",
    "            if racecourse_list[racecourse_i] == \"01\":\n",
    "                place = \"札幌\"\n",
    "            elif racecourse_list[racecourse_i] == \"02\":\n",
    "                place = \"函館\"\n",
    "            elif racecourse_list[racecourse_i] == \"03\":\n",
    "                place = \"福島\"\n",
    "            elif racecourse_list[racecourse_i] == \"04\":\n",
    "                place = \"新潟\"\n",
    "            elif racecourse_list[racecourse_i] == \"05\":\n",
    "                place = \"東京\"\n",
    "            elif racecourse_list[racecourse_i] == \"06\":\n",
    "                place = \"中山\"\n",
    "            elif racecourse_list[racecourse_i] == \"07\":\n",
    "                place = \"中京\"\n",
    "            elif racecourse_list[racecourse_i] == \"08\":\n",
    "                place = \"京都\"\n",
    "            elif racecourse_list[racecourse_i] == \"09\":\n",
    "                place = \"阪神\"\n",
    "            elif racecourse_list[racecourse_i] == \"10\":\n",
    "                place = \"小倉\"    \n",
    "            # ②開催回数ごとにループ（6回）\n",
    "            for session_number_i in range(6+1):\n",
    "                # ③開催日数分ループ（12日）\n",
    "                for event_date_i in range(12+1):\n",
    "                    race_id = ''\n",
    "                    if event_date_i<9:\n",
    "                        race_id = str(year)+racecourse_list[racecourse_i]+\"0\"+str(session_number_i+1)+\"0\"+str(event_date_i+1)\n",
    "                        url1=\"https://db.netkeiba.com/race/\"+race_id\n",
    "                    else:\n",
    "                        race_id = str(year)+racecourse_list[racecourse_i]+\"0\"+str(session_number_i+1)+\"0\"+str(event_date_i+1)\n",
    "                        url1=\"https://db.netkeiba.com/race/\"+race_id\n",
    "                    # event_date_iの更新をbreakするためのカウンター\n",
    "                    event_date_i_BreakCounter = 0\n",
    "                    # ④レース数分ループ（12R）\n",
    "                    for race_number_i in range(12):\n",
    "                        if race_number_i<9:\n",
    "                            url=url1+str(\"0\")+str(race_number_i+1)\n",
    "                            current_race_id = race_id+str(\"0\")+str(race_number_i+1)\n",
    "                        else:\n",
    "                            url=url1+str(race_number_i+1)\n",
    "                            current_race_id = race_id+str(race_number_i+1)\n",
    "                        # add the URL to the DataFrame\n",
    "                        netkeiba_url_potential_df.loc[len(netkeiba_url_potential_df)] = [url, current_race_id, racecourse_i, place, race_number_i, year]\n",
    "    print(f'Length of netkeiba_url_potential_df: {len(netkeiba_url_potential_df)}')\n",
    "    return netkeiba_url_potential_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得開始年\n",
    "year_start = 2019\n",
    "#取得終了年（含む）\n",
    "year_end = 2019\n",
    "netkeiba_url_potential_df = create_netkeiba_url_potential(year_start, year_end)\n",
    "netkeiba_url_potential_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_bq_to_gcs(dataset_id, table_id, column_names, gcs_bucket, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Export a BigQuery table to a Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        dataset_id (str): The ID of the BigQuery dataset.\n",
    "        table_id (str): The ID of the BigQuery table.\n",
    "        column_names (list): A list of column names to export from the table.\n",
    "        gcs_bucket (str): The name of the Google Cloud Storage bucket.\n",
    "        destination_blob_name (str): The name of the destination blob in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize BigQuery and Storage clients\n",
    "    bq_client = bigquery.Client()\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Specify the BigQuery table\n",
    "    table_ref = bq_client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Set up the query (will only get the 'column_names' from the table)\n",
    "    # query = f\"SELECT {', '.join(column_names)} FROM `{table_ref.dataset_id}.{table_ref.table_id}`\"\n",
    "    query = f\"SELECT {', '.join([f'`{name}`' for name in column_names])} FROM `{table_ref.dataset_id}.{table_ref.table_id}`\"\n",
    "\n",
    "    # Execute the query and convert the results to a pandas DataFrame\n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    # Save the DataFrame to a csv file\n",
    "    df.to_csv('temp.csv', index=False)\n",
    "\n",
    "    # Specify the GCS bucket\n",
    "    bucket = storage_client.get_bucket(gcs_bucket)\n",
    "\n",
    "    # Name of the destination blob\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Upload the local file to the bucket\n",
    "    blob.upload_from_filename('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_bq_to_gcs(\"kh\", \"netkeiba_url_scraped\", ['url'], \"dev-kh-gcs-bucket\", \"data/netkeiba_url_scraped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_read_netkeiba_url_scraped_csv_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"\n",
    "    Download a CSV file from a GCS bucket and read it into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    bucket_name (str): The name of the GCS bucket.\n",
    "    source_blob_name (str): The name of the blob in the GCS bucket.\n",
    "    destination_file_name (str): The name of the file to save the downloaded CSV.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the data from the downloaded CSV file.\n",
    "    \"\"\"\n",
    "    # Create a client\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    # Get the blob\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if blob.exists():\n",
    "        # Download the file\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "\n",
    "        # If the file exists, read it into a DataFrame\n",
    "        if os.path.exists(destination_file_name):\n",
    "            netkeiba_url_scraped_df = pd.read_csv(destination_file_name)\n",
    "            print(f'Length of netkeiba_url_scraped_df: {len(netkeiba_url_scraped_df)}')\n",
    "            return netkeiba_url_scraped_df\n",
    "    else:\n",
    "        print(f\"The file {source_blob_name} does not exist in the bucket {bucket_name}.\")\n",
    "        # Return an empty DataFrame\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netkeiba_url_scraped_df = download_read_netkeiba_url_scraped_csv_from_gcs(\"dev-kh-gcs-bucket\", \"data/netkeiba_url_scraped.csv\", \"netkeiba_url_scraped.csv\")\n",
    "netkeiba_url_scraped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_scraped_urls(netkeiba_url_potential_df, netkeiba_url_scraped_df):\n",
    "    \"\"\"\n",
    "    Remove scraped URLs from the potential URLs DataFrame.\n",
    "\n",
    "    Args:\n",
    "        netkeiba_url_potential_df (pd.DataFrame): DataFrame containing the potential URLs.\n",
    "        netkeiba_url_scraped_df (pd.DataFrame): DataFrame containing the scraped URLs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the scraped URLs removed.\n",
    "    \"\"\"\n",
    "    # Check if netkeiba_url_scraped_df is not None and not empty\n",
    "    if netkeiba_url_scraped_df is not None and not netkeiba_url_scraped_df.empty:\n",
    "        # Remove rows in netkeiba_url_potential_df that are also in netkeiba_url_scraped_df\n",
    "        netkeiba_url_unique_df = netkeiba_url_potential_df[~netkeiba_url_potential_df['url'].isin(netkeiba_url_scraped_df['url'])]\n",
    "        print(f'Length of netkeiba_url_unique_df: {len(netkeiba_url_unique_df)}')\n",
    "        return netkeiba_url_unique_df\n",
    "    else:\n",
    "        print(\"netkeiba_url_scraped_df is None or empty. Returning netkeiba_url_potential_df as is.\")\n",
    "        return netkeiba_url_potential_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netkeiba_url_scraped_df is None or empty. Returning netkeiba_url_potential_df as is.\n"
     ]
    }
   ],
   "source": [
    "netkeiba_url_unique_df = remove_scraped_urls(netkeiba_url_potential_df, netkeiba_url_scraped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_netkeiba_data(netkeiba_url_unique_df):\n",
    "    # Initialize a list to store the responses\n",
    "    race_data_all = []\n",
    "    #取得するデータのヘッダー情報を先に追加しておく\n",
    "    race_data_all.append(['race_id','馬','騎手','馬番','走破時間','オッズ','通過順','着順','体重','体重変化','性','齢','斤量','上がり','人気','レース名','日付','開催','クラス','芝orダート','距離','回り','馬場','天気','場id','場名'])\n",
    "\n",
    "    # Loop over each row in the DataFrame\n",
    "    for _, row in netkeiba_url_unique_df.iterrows():\n",
    "        print(f'Processing loop {_} of {len(netkeiba_url_unique_df)}')\n",
    "        url = row['url']\n",
    "        current_race_id = row['current_race_id']\n",
    "        racecourse_i = row['racecourse_i']\n",
    "        place = row['place']\n",
    "        race_number_i = row['race_number_i']\n",
    "        year = row['year']\n",
    "        try:\n",
    "            r=requests.get(url)\n",
    "        #リクエストを投げすぎるとエラーになることがあるため\n",
    "        #失敗したら10秒待機してリトライする\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Retrying in 10 seconds...\")\n",
    "            time.sleep(10)  # 10秒待機\n",
    "            r=requests.get(url)\n",
    "        #バグ対策でdecode\n",
    "        soup = BeautifulSoup(r.content.decode(\"euc-jp\", \"ignore\"), \"html.parser\")\n",
    "        soup_span = soup.find_all(\"span\")\n",
    "        # テーブルを指定\n",
    "        main_table = soup.find(\"table\", {\"class\": \"race_table_01 nk_tb_common\"})\n",
    "    \n",
    "        # テーブル内の全ての行を取得\n",
    "        try:\n",
    "            main_rows = main_table.find_all(\"tr\")\n",
    "        except:\n",
    "            print('continue: ' + url)\n",
    "            continue\n",
    "    \n",
    "        race_data = []\n",
    "        for i, row in enumerate(main_rows[1:], start=1):# ヘッダ行をスキップ\n",
    "            cols = row.find_all(\"td\")\n",
    "            #走破時間\n",
    "            runtime=''\n",
    "            try:\n",
    "                runtime= cols[7].text.strip()\n",
    "            except IndexError:\n",
    "                runtime = ''\n",
    "            soup_nowrap = soup.find_all(\"td\",nowrap=\"nowrap\",class_=None)\n",
    "            #通過順\n",
    "            pas = ''\n",
    "            try:\n",
    "                pas = str(cols[10].text.strip())\n",
    "            except:\n",
    "                pas = ''\n",
    "            weight = 0\n",
    "            weight_dif = 0\n",
    "            #体重\n",
    "            var = cols[14].text.strip()\n",
    "            try:\n",
    "                weight = int(var.split(\"(\")[0])\n",
    "                weight_dif = int(var.split(\"(\")[1][0:-1])\n",
    "            except ValueError:\n",
    "                weight = 0\n",
    "                weight_dif = 0\n",
    "            weight = weight\n",
    "            weight_dif = weight_dif\n",
    "            #上がり\n",
    "            last = ''\n",
    "            try:\n",
    "                last = cols[11].text.strip()\n",
    "            except IndexError:\n",
    "                last = ''\n",
    "            #人気\n",
    "            pop = ''\n",
    "            try:\n",
    "                pop = cols[13].text.strip()\n",
    "            except IndexError:\n",
    "                pop = ''\n",
    "            \n",
    "            #レースの情報\n",
    "            try:\n",
    "                var = soup_span[8]\n",
    "                sur=str(var).split(\"/\")[0].split(\">\")[1][0]\n",
    "                rou=str(var).split(\"/\")[0].split(\">\")[1][1]\n",
    "                dis=str(var).split(\"/\")[0].split(\">\")[1].split(\"m\")[0][-4:]\n",
    "                con=str(var).split(\"/\")[2].split(\":\")[1][1]\n",
    "                wed=str(var).split(\"/\")[1].split(\":\")[1][1]\n",
    "            except IndexError:\n",
    "                try:\n",
    "                    var = soup_span[7]\n",
    "                    sur=str(var).split(\"/\")[0].split(\">\")[1][0]\n",
    "                    rou=str(var).split(\"/\")[0].split(\">\")[1][1]\n",
    "                    dis=str(var).split(\"/\")[0].split(\">\")[1].split(\"m\")[0][-4:]\n",
    "                    con=str(var).split(\"/\")[2].split(\":\")[1][1]\n",
    "                    wed=str(var).split(\"/\")[1].split(\":\")[1][1]\n",
    "                except IndexError:\n",
    "                    var = soup_span[6]\n",
    "                    sur=str(var).split(\"/\")[0].split(\">\")[1][0]\n",
    "                    rou=str(var).split(\"/\")[0].split(\">\")[1][1]\n",
    "                    dis=str(var).split(\"/\")[0].split(\">\")[1].split(\"m\")[0][-4:]\n",
    "                    con=str(var).split(\"/\")[2].split(\":\")[1][1]\n",
    "                    wed=str(var).split(\"/\")[1].split(\":\")[1][1]\n",
    "            soup_smalltxt = soup.find_all(\"p\",class_=\"smalltxt\")\n",
    "            detail=str(soup_smalltxt).split(\">\")[1].split(\" \")[1]\n",
    "            date=str(soup_smalltxt).split(\">\")[1].split(\" \")[0]\n",
    "            clas=str(soup_smalltxt).split(\">\")[1].split(\" \")[2].replace(u'\\xa0', u' ').split(\" \")[0]\n",
    "            title=str(soup.find_all(\"h1\")[1]).split(\">\")[1].split(\"<\")[0]\n",
    "    \n",
    "            race_data = [\n",
    "                current_race_id,\n",
    "                cols[3].text.strip(),#馬の名前\n",
    "                cols[6].text.strip(),#騎手の名前\n",
    "                cols[2].text.strip(),#馬番\n",
    "                runtime,#走破時間\n",
    "                cols[12].text.strip(),#オッズ,\n",
    "                pas,#通過順\n",
    "                cols[0].text.strip(),#着順\n",
    "                weight,#体重\n",
    "                weight_dif,#体重変化\n",
    "                cols[4].text.strip()[0],#性\n",
    "                cols[4].text.strip()[1],#齢\n",
    "                cols[5].text.strip(),#斤量\n",
    "                last,#上がり\n",
    "                pop,#人気,\n",
    "                title,#レース名\n",
    "                date,#日付\n",
    "                detail,\n",
    "                clas,#クラス\n",
    "                sur,#芝かダートか\n",
    "                dis,#距離\n",
    "                rou,#回り\n",
    "                con,#馬場状態\n",
    "                wed,#天気\n",
    "                racecourse_i,#場\n",
    "                place]\n",
    "            # Append the list to the race_data_all\n",
    "            race_data_all.append(race_data)\n",
    "    return race_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing loop 0 of 1092\n",
      "Processing loop 1 of 1092\n",
      "Processing loop 2 of 1092\n",
      "Processing loop 3 of 1092\n",
      "Processing loop 4 of 1092\n",
      "Processing loop 5 of 1092\n",
      "Processing loop 6 of 1092\n",
      "Processing loop 7 of 1092\n",
      "Processing loop 8 of 1092\n",
      "Processing loop 9 of 1092\n",
      "Processing loop 10 of 1092\n",
      "Processing loop 11 of 1092\n",
      "Processing loop 12 of 1092\n",
      "Processing loop 13 of 1092\n",
      "Processing loop 14 of 1092\n",
      "Processing loop 15 of 1092\n",
      "Processing loop 16 of 1092\n",
      "Processing loop 17 of 1092\n",
      "Processing loop 18 of 1092\n",
      "Processing loop 19 of 1092\n",
      "Processing loop 20 of 1092\n",
      "Processing loop 21 of 1092\n",
      "Processing loop 22 of 1092\n",
      "Processing loop 23 of 1092\n",
      "Processing loop 24 of 1092\n",
      "Processing loop 25 of 1092\n",
      "Processing loop 26 of 1092\n",
      "Processing loop 27 of 1092\n",
      "Processing loop 28 of 1092\n",
      "Processing loop 29 of 1092\n",
      "Processing loop 30 of 1092\n",
      "Processing loop 31 of 1092\n",
      "Processing loop 32 of 1092\n",
      "Processing loop 33 of 1092\n",
      "Processing loop 34 of 1092\n",
      "Processing loop 35 of 1092\n",
      "Processing loop 36 of 1092\n",
      "Processing loop 37 of 1092\n",
      "Processing loop 38 of 1092\n",
      "Processing loop 39 of 1092\n",
      "Processing loop 40 of 1092\n",
      "Processing loop 41 of 1092\n",
      "Processing loop 42 of 1092\n",
      "Processing loop 43 of 1092\n",
      "Processing loop 44 of 1092\n",
      "Processing loop 45 of 1092\n",
      "Processing loop 46 of 1092\n",
      "Processing loop 47 of 1092\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/mnt/netkeiba_scraper.ipynb セル 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f796f75746866756c5f72686f646573227d/opt/mnt/netkeiba_scraper.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m race_data_all \u001b[39m=\u001b[39m scrape_netkeiba_data(netkeiba_url_unique_df)\n",
      "\u001b[1;32m/opt/mnt/netkeiba_scraper.ipynb セル 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f796f75746866756c5f72686f646573227d/opt/mnt/netkeiba_scraper.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m year \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f796f75746866756c5f72686f646573227d/opt/mnt/netkeiba_scraper.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f796f75746866756c5f72686f646573227d/opt/mnt/netkeiba_scraper.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     r\u001b[39m=\u001b[39mrequests\u001b[39m.\u001b[39;49mget(url)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f796f75746866756c5f72686f646573227d/opt/mnt/netkeiba_scraper.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#リクエストを投げすぎるとエラーになることがあるため\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f796f75746866756c5f72686f646573227d/opt/mnt/netkeiba_scraper.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#失敗したら10秒待機してリトライする\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f796f75746866756c5f72686f646573227d/opt/mnt/netkeiba_scraper.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    716\u001b[0m     conn,\n\u001b[1;32m    717\u001b[0m     method,\n\u001b[1;32m    718\u001b[0m     url,\n\u001b[1;32m    719\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    720\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    721\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    722\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    723\u001b[0m )\n\u001b[1;32m    725\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    468\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:1390\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1390\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1391\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1392\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    326\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "race_data_all = scrape_netkeiba_data(netkeiba_url_unique_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race_id',\n",
       " '馬',\n",
       " '騎手',\n",
       " '馬番',\n",
       " '走破時間',\n",
       " 'オッズ',\n",
       " '通過順',\n",
       " '着順',\n",
       " '体重',\n",
       " '体重変化',\n",
       " '性',\n",
       " '齢',\n",
       " '斤量',\n",
       " '上がり',\n",
       " '人気',\n",
       " 'レース名',\n",
       " '日付',\n",
       " '開催',\n",
       " 'クラス',\n",
       " '芝orダート',\n",
       " '距離',\n",
       " '回り',\n",
       " '馬場',\n",
       " '天気',\n",
       " '場id',\n",
       " '場名']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_data_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_to_bigquery(df, table_id, project_id):\n",
    "    # pandas_gbq.to_gbq(df, table_id, project_id, if_exists='append')\n",
    "    pandas_gbq.to_gbq(df, table_id, project_id=project_id, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = race_data_all[0]\n",
    "race_data_all_df = pd.DataFrame(race_data_all[1:], columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race_id',\n",
       " '馬',\n",
       " '騎手',\n",
       " '馬番',\n",
       " '走破時間',\n",
       " 'オッズ',\n",
       " '通過順',\n",
       " '着順',\n",
       " '体重',\n",
       " '体重変化',\n",
       " '性',\n",
       " '齢',\n",
       " '斤量',\n",
       " '上がり',\n",
       " '人気',\n",
       " 'レース名',\n",
       " '日付',\n",
       " '開催',\n",
       " 'クラス',\n",
       " '芝・ダート',\n",
       " '距離',\n",
       " '回り',\n",
       " '馬場',\n",
       " '天気',\n",
       " '場id',\n",
       " '場名']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df_to_bigquery(race_data_all_df, 'kh.raw_netkeiba_race', 'keiba-hacke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df_to_bigquery(netkeiba_url_unique_df, 'kh.netkeiba_url_scraped', 'keiba-hacke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
